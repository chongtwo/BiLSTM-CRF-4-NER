{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "import codecs\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.utils import to_categorical \n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Embedding, LSTM, Dense, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "from keras.models import load_model\n",
    "from keras_contrib.layers.crf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一、数据预处理\n",
    "# BASE_DIR = 'F:\\PythonProject\\BiLSTM-CRF-4-NER'  # my DELL\n",
    "BASE_DIR='E:\\PycharmProjects\\BiLSTM-CRF-4-NER' # LAB\n",
    "TRAIN_TEXT_DIR = BASE_DIR + '/data/train'\n",
    "TEST_TEXT_DIR = BASE_DIR + '/data/test'\n",
    "MAX_SEQUENCE_LENGTH = 100 # 一份文本中最大字数\n",
    "MAX_NB_WORDS = 20000 # 全部文本中最大unique字数\n",
    "VALIDATION_SPLIT = 0.2\n",
    "batch_size = 32\n",
    "labels_index = {'B-nr':1, 'B-ns':2, 'B-nt':3, 'I-nr':4, 'I-ns':5, 'I-nt':6, 'O':0} # 标签名-id 字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.加载预训练好的embedding模型——字：向量 词典\n",
    "char_model = word2vec.Word2Vec.load(\"char_model.model\")\n",
    "word_vector_size = char_model.wv.vector_size\n",
    "vocab_size = len(char_model.wv.vocab) # 4767"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4767"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Word2VecKeyedVectors.get_keras_embedding of <gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x00000190CD219A20>>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_model.wv.get_keras_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 预备文本和标签\n",
    "def make_list(file_path):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    sentence = []\n",
    "    sentence_labels = []\n",
    "    f = open(file_path, encoding='utf8')\n",
    "    for line in f:\n",
    "        if(line != \"\\n\"):\n",
    "            values = line.split(\"\\t\");\n",
    "            char = values[0]\n",
    "            label = values[1].replace(\"\\n\",\"\")\n",
    "            sentence.append(char)\n",
    "            sentence_labels.append(labels_index.get(label, \"0\"))\n",
    "        else:\n",
    "            texts.append(sentence)\n",
    "            labels.append(sentence_labels)\n",
    "            sentence=[]\n",
    "            sentence_labels=[]\n",
    "    f.close()\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels = make_list(TRAIN_TEXT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels[0]),len(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.建字表——字：id 词典\n",
    "tokenizer = Tokenizer(num_words = min(vocab_size,MAX_NB_WORDS))\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index # 4652"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(word_index) # 4652"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(sequences[0]),len(sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (46364, 100)\n",
      "Shape of label tensor: (46364, 100, 7)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = pad_sequences(labels, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels[0]),len(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据分成训练集和验证集\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples=int(VALIDATION_SPLIT * data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37092, 100)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37092, 100, 7)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9272, 9272)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_val),len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4653, 100)\n"
     ]
    }
   ],
   "source": [
    "# 生成embedding_matrix = id-向量 表\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index)) \n",
    "embedding_matrix = np.zeros((nb_words+1, word_vector_size))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = char_model.wv.get_vector(word)\n",
    "    except KeyError as e:\n",
    "        embedding_vecotr = np.zeros(word_vector_size)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM构建\n",
    "embedding_layer = Embedding(nb_words+1,\n",
    "                            word_vector_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False\n",
    "                            )\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(100, dropout=0.2, return_sequences=True))\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "model.add(Dense(len(labels_index),activation='softmax'))\n",
    "crf_layer = CRF(len(labels_index))\n",
    "model.add(crf_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 100)          465300    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100, 100)          80400     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100, 100)          10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100, 7)            707       \n",
      "_________________________________________________________________\n",
      "crf_7 (CRF)                  (None, 100, 7)            119       \n",
      "=================================================================\n",
      "Total params: 556,626\n",
      "Trainable params: 91,326\n",
      "Non-trainable params: 465,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37092 samples, validate on 9272 samples\n",
      "Epoch 1/100\n",
      "37092/37092 [==============================] - 85s 2ms/step - loss: 0.7967 - acc: 0.9218 - val_loss: 0.3868 - val_acc: 0.9642\n",
      "Epoch 2/100\n",
      "37092/37092 [==============================] - 82s 2ms/step - loss: 0.2546 - acc: 0.9637 - val_loss: 0.1923 - val_acc: 0.9642\n",
      "Epoch 3/100\n",
      "37092/37092 [==============================] - 82s 2ms/step - loss: 0.1819 - acc: 0.9637 - val_loss: 0.1542 - val_acc: 0.9642\n",
      "Epoch 4/100\n",
      "37092/37092 [==============================] - 83s 2ms/step - loss: 0.1173 - acc: 0.9637 - val_loss: 0.0980 - val_acc: 0.9642\n",
      "Epoch 5/100\n",
      "37092/37092 [==============================] - 82s 2ms/step - loss: 0.0884 - acc: 0.9647 - val_loss: 0.0764 - val_acc: 0.9679\n",
      "Epoch 6/100\n",
      "37092/37092 [==============================] - 82s 2ms/step - loss: 0.0712 - acc: 0.9694 - val_loss: 0.0627 - val_acc: 0.9720\n",
      "Epoch 7/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: 0.0595 - acc: 0.9733 - val_loss: 0.0531 - val_acc: 0.9758\n",
      "Epoch 8/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: 0.0512 - acc: 0.9758 - val_loss: 0.0460 - val_acc: 0.9791\n",
      "Epoch 9/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: 0.0448 - acc: 0.9783 - val_loss: 0.0433 - val_acc: 0.9765\n",
      "Epoch 10/100\n",
      "37092/37092 [==============================] - 82s 2ms/step - loss: 0.0397 - acc: 0.9790 - val_loss: 0.0358 - val_acc: 0.9805\n",
      "Epoch 11/100\n",
      "37092/37092 [==============================] - 82s 2ms/step - loss: 0.0353 - acc: 0.9796 - val_loss: 0.0319 - val_acc: 0.9809\n",
      "Epoch 12/100\n",
      "37092/37092 [==============================] - 82s 2ms/step - loss: 0.0313 - acc: 0.9802 - val_loss: 0.0284 - val_acc: 0.9813\n",
      "Epoch 13/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: 0.0276 - acc: 0.9806 - val_loss: 0.0259 - val_acc: 0.9805\n",
      "Epoch 14/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: 0.0239 - acc: 0.9811 - val_loss: 0.0215 - val_acc: 0.9815\n",
      "Epoch 15/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: 0.0207 - acc: 0.9813 - val_loss: 0.0189 - val_acc: 0.9816\n",
      "Epoch 16/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: 0.0174 - acc: 0.9818 - val_loss: 0.0156 - val_acc: 0.9814\n",
      "Epoch 17/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: 0.0141 - acc: 0.9821 - val_loss: 0.0118 - val_acc: 0.9830\n",
      "Epoch 18/100\n",
      "37092/37092 [==============================] - 82s 2ms/step - loss: 0.0109 - acc: 0.9824 - val_loss: 0.0088 - val_acc: 0.9832\n",
      "Epoch 19/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: 0.0078 - acc: 0.9826 - val_loss: 0.0062 - val_acc: 0.9824\n",
      "Epoch 20/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: 0.0046 - acc: 0.9827 - val_loss: 0.0030 - val_acc: 0.9831\n",
      "Epoch 21/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: 0.0016 - acc: 0.9831 - val_loss: 2.9834e-04 - val_acc: 0.9832\n",
      "Epoch 22/100\n",
      "37092/37092 [==============================] - 85s 2ms/step - loss: -0.0016 - acc: 0.9833 - val_loss: -0.0033 - val_acc: 0.9835\n",
      "Epoch 23/100\n",
      "37092/37092 [==============================] - 82s 2ms/step - loss: -0.0048 - acc: 0.9834 - val_loss: -0.0062 - val_acc: 0.9838\n",
      "Epoch 24/100\n",
      "37092/37092 [==============================] - 83s 2ms/step - loss: -0.0079 - acc: 0.9837 - val_loss: -0.0093 - val_acc: 0.9839\n",
      "Epoch 25/100\n",
      "37092/37092 [==============================] - 84s 2ms/step - loss: -0.0110 - acc: 0.9840 - val_loss: -0.0126 - val_acc: 0.9846\n",
      "Epoch 26/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.0141 - acc: 0.9845 - val_loss: -0.0156 - val_acc: 0.9850\n",
      "Epoch 27/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.0173 - acc: 0.9848 - val_loss: -0.0185 - val_acc: 0.9852\n",
      "Epoch 28/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.0202 - acc: 0.9851 - val_loss: -0.0214 - val_acc: 0.9856\n",
      "Epoch 29/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.0233 - acc: 0.9853 - val_loss: -0.0243 - val_acc: 0.9858\n",
      "Epoch 30/100\n",
      "37092/37092 [==============================] - 82s 2ms/step - loss: -0.0263 - acc: 0.9855 - val_loss: -0.0271 - val_acc: 0.9859\n",
      "Epoch 31/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.0292 - acc: 0.9857 - val_loss: -0.0300 - val_acc: 0.9855\n",
      "Epoch 32/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.0322 - acc: 0.9859 - val_loss: -0.0330 - val_acc: 0.9861\n",
      "Epoch 33/100\n",
      "37092/37092 [==============================] - 82s 2ms/step - loss: -0.0351 - acc: 0.9860 - val_loss: -0.0360 - val_acc: 0.9863\n",
      "Epoch 34/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.0381 - acc: 0.9862 - val_loss: -0.0386 - val_acc: 0.9859\n",
      "Epoch 35/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.0409 - acc: 0.9863 - val_loss: -0.0414 - val_acc: 0.9861\n",
      "Epoch 36/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.0439 - acc: 0.9865 - val_loss: -0.0440 - val_acc: 0.9861\n",
      "Epoch 37/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.0467 - acc: 0.9866 - val_loss: -0.0475 - val_acc: 0.9868\n",
      "Epoch 38/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.0496 - acc: 0.9867 - val_loss: -0.0500 - val_acc: 0.9866\n",
      "Epoch 39/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.0525 - acc: 0.9867 - val_loss: -0.0530 - val_acc: 0.9869\n",
      "Epoch 40/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.0553 - acc: 0.9869 - val_loss: -0.0547 - val_acc: 0.9862\n",
      "Epoch 41/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.0583 - acc: 0.9870 - val_loss: -0.0581 - val_acc: 0.9862\n",
      "Epoch 42/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.0611 - acc: 0.9872 - val_loss: -0.0613 - val_acc: 0.9869\n",
      "Epoch 43/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.0638 - acc: 0.9872 - val_loss: -0.0643 - val_acc: 0.9871\n",
      "Epoch 44/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.0668 - acc: 0.9873 - val_loss: -0.0671 - val_acc: 0.9872\n",
      "Epoch 45/100\n",
      "37092/37092 [==============================] - 82s 2ms/step - loss: -0.0697 - acc: 0.9876 - val_loss: -0.0698 - val_acc: 0.9873\n",
      "Epoch 46/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.0725 - acc: 0.9875 - val_loss: -0.0726 - val_acc: 0.9872\n",
      "Epoch 47/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.0753 - acc: 0.9875 - val_loss: -0.0755 - val_acc: 0.9875\n",
      "Epoch 48/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.0781 - acc: 0.9878 - val_loss: -0.0781 - val_acc: 0.9874\n",
      "Epoch 49/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.0809 - acc: 0.9877 - val_loss: -0.0810 - val_acc: 0.9876\n",
      "Epoch 50/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.0837 - acc: 0.9878 - val_loss: -0.0837 - val_acc: 0.9876\n",
      "Epoch 51/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.0865 - acc: 0.9878 - val_loss: -0.0865 - val_acc: 0.9876\n",
      "Epoch 52/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.0894 - acc: 0.9878 - val_loss: -0.0892 - val_acc: 0.9877\n",
      "Epoch 53/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.0922 - acc: 0.9881 - val_loss: -0.0911 - val_acc: 0.9870\n",
      "Epoch 54/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.0949 - acc: 0.9879 - val_loss: -0.0944 - val_acc: 0.9875\n",
      "Epoch 55/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.0978 - acc: 0.9882 - val_loss: -0.0975 - val_acc: 0.9878\n",
      "Epoch 56/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.1005 - acc: 0.9881 - val_loss: -0.1001 - val_acc: 0.9877\n",
      "Epoch 57/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.1030 - acc: 0.9876 - val_loss: -0.1026 - val_acc: 0.9877\n",
      "Epoch 58/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.1059 - acc: 0.9882 - val_loss: -0.1056 - val_acc: 0.9879\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.1087 - acc: 0.9883 - val_loss: -0.1079 - val_acc: 0.9878\n",
      "Epoch 60/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.1114 - acc: 0.9883 - val_loss: -0.1099 - val_acc: 0.9867\n",
      "Epoch 61/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.1133 - acc: 0.9880 - val_loss: -0.1120 - val_acc: 0.9872\n",
      "Epoch 62/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.1167 - acc: 0.9884 - val_loss: -0.1163 - val_acc: 0.9879\n",
      "Epoch 63/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.1197 - acc: 0.9884 - val_loss: -0.1192 - val_acc: 0.9880\n",
      "Epoch 64/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.1226 - acc: 0.9886 - val_loss: -0.1219 - val_acc: 0.9880\n",
      "Epoch 65/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.1253 - acc: 0.9885 - val_loss: -0.1242 - val_acc: 0.9876\n",
      "Epoch 66/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.1282 - acc: 0.9886 - val_loss: -0.1262 - val_acc: 0.9867\n",
      "Epoch 67/100\n",
      "37092/37092 [==============================] - 85s 2ms/step - loss: -0.1310 - acc: 0.9887 - val_loss: -0.1295 - val_acc: 0.9874\n",
      "Epoch 68/100\n",
      "37092/37092 [==============================] - 82s 2ms/step - loss: -0.1338 - acc: 0.9887 - val_loss: -0.1327 - val_acc: 0.9877\n",
      "Epoch 69/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.1366 - acc: 0.9888 - val_loss: -0.1355 - val_acc: 0.9878\n",
      "Epoch 70/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.1393 - acc: 0.9888 - val_loss: -0.1365 - val_acc: 0.9860\n",
      "Epoch 71/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.1422 - acc: 0.9889 - val_loss: -0.1411 - val_acc: 0.9879\n",
      "Epoch 72/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.1449 - acc: 0.9888 - val_loss: -0.1434 - val_acc: 0.9878\n",
      "Epoch 73/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.1477 - acc: 0.9890 - val_loss: -0.1468 - val_acc: 0.9882\n",
      "Epoch 74/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.1505 - acc: 0.9889 - val_loss: -0.1495 - val_acc: 0.9878\n",
      "Epoch 75/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.1532 - acc: 0.9888 - val_loss: -0.1521 - val_acc: 0.9880\n",
      "Epoch 76/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.1560 - acc: 0.9890 - val_loss: -0.1551 - val_acc: 0.9882\n",
      "Epoch 77/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.1588 - acc: 0.9890 - val_loss: -0.1576 - val_acc: 0.9882\n",
      "Epoch 78/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.1616 - acc: 0.9892 - val_loss: -0.1604 - val_acc: 0.9882\n",
      "Epoch 79/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.1645 - acc: 0.9892 - val_loss: -0.1630 - val_acc: 0.9882\n",
      "Epoch 80/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.1671 - acc: 0.9890 - val_loss: -0.1648 - val_acc: 0.9876\n",
      "Epoch 81/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.1699 - acc: 0.9891 - val_loss: -0.1685 - val_acc: 0.9882\n",
      "Epoch 82/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.1727 - acc: 0.9891 - val_loss: -0.1710 - val_acc: 0.9882\n",
      "Epoch 83/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.1754 - acc: 0.9891 - val_loss: -0.1742 - val_acc: 0.9883\n",
      "Epoch 84/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.1783 - acc: 0.9892 - val_loss: -0.1769 - val_acc: 0.9880\n",
      "Epoch 85/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.1811 - acc: 0.9893 - val_loss: -0.1797 - val_acc: 0.9884\n",
      "Epoch 86/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.1837 - acc: 0.9893 - val_loss: -0.1824 - val_acc: 0.9884\n",
      "Epoch 87/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.1860 - acc: 0.9890 - val_loss: -0.1839 - val_acc: 0.9879\n",
      "Epoch 88/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.1891 - acc: 0.9893 - val_loss: -0.1878 - val_acc: 0.9883\n",
      "Epoch 89/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.1921 - acc: 0.9895 - val_loss: -0.1899 - val_acc: 0.9876\n",
      "Epoch 90/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.1948 - acc: 0.9893 - val_loss: -0.1934 - val_acc: 0.9884\n",
      "Epoch 91/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.1976 - acc: 0.9893 - val_loss: -0.1959 - val_acc: 0.9882\n",
      "Epoch 92/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.2003 - acc: 0.9894 - val_loss: -0.1979 - val_acc: 0.9879\n",
      "Epoch 93/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.2032 - acc: 0.9896 - val_loss: -0.2012 - val_acc: 0.9881\n",
      "Epoch 94/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.2060 - acc: 0.9895 - val_loss: -0.2039 - val_acc: 0.9882\n",
      "Epoch 95/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.2087 - acc: 0.9895 - val_loss: -0.2071 - val_acc: 0.9883\n",
      "Epoch 96/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.2114 - acc: 0.9894 - val_loss: -0.2090 - val_acc: 0.9878\n",
      "Epoch 97/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.2143 - acc: 0.9896 - val_loss: -0.2116 - val_acc: 0.9879\n",
      "Epoch 98/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.2167 - acc: 0.9894 - val_loss: -0.2139 - val_acc: 0.9874\n",
      "Epoch 99/100\n",
      "37092/37092 [==============================] - 80s 2ms/step - loss: -0.2199 - acc: 0.9897 - val_loss: -0.2181 - val_acc: 0.9885\n",
      "Epoch 100/100\n",
      "37092/37092 [==============================] - 81s 2ms/step - loss: -0.2226 - acc: 0.9896 - val_loss: -0.2206 - val_acc: 0.9884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x190cee33c18>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 六、LSTM训练\n",
    "# 1. 编译\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "model.compile('rmsprop', loss=crf_layer.loss_function, metrics=[crf_layer.accuracy])\n",
    "# 2.拟合\n",
    "model.fit(x_train, y_train, batch_size = 128, epochs = 100,\n",
    "         validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_contrib.utils import save_load_utils\n",
    "filename = 'BiLSTM_CRF.h5'\n",
    "save_load_utils.save_all_weights(model,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9272, 37092)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_val), len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9272/9272 [==============================] - 10s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.22058069649392723, 0.9883930103546387]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_data(input_file):\n",
    "        texts = []\n",
    "        input_data = codecs.open(input_file, 'r', 'utf-8')\n",
    "        for line in input_data.readlines():\n",
    "            words=[]\n",
    "            for word in line:\n",
    "                words.append(word)\n",
    "            texts.append(words)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = make_test_data(TEST_TEXT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 172,\n",
       "       167, 127, 126,   5, 172, 167,  72, 403, 135])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = model.predict(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_predict[32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"本报波士顿１１月１日电记者陈特安、李云飞报道：江泽民主席一行今天上午乘专机从纽约抵达波士顿访问。\"\n",
    "texts = []\n",
    "words=[]\n",
    "for word in line:\n",
    "    words.append(word)\n",
    "texts.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['本',\n",
       "  '报',\n",
       "  '波',\n",
       "  '士',\n",
       "  '顿',\n",
       "  '１',\n",
       "  '１',\n",
       "  '月',\n",
       "  '１',\n",
       "  '日',\n",
       "  '电',\n",
       "  '记',\n",
       "  '者',\n",
       "  '陈',\n",
       "  '特',\n",
       "  '安',\n",
       "  '、',\n",
       "  '李',\n",
       "  '云',\n",
       "  '飞',\n",
       "  '报',\n",
       "  '道',\n",
       "  '：',\n",
       "  '江',\n",
       "  '泽',\n",
       "  '民',\n",
       "  '主',\n",
       "  '席',\n",
       "  '一',\n",
       "  '行',\n",
       "  '今',\n",
       "  '天',\n",
       "  '上',\n",
       "  '午',\n",
       "  '乘',\n",
       "  '专',\n",
       "  '机',\n",
       "  '从',\n",
       "  '纽',\n",
       "  '约',\n",
       "  '抵',\n",
       "  '达',\n",
       "  '波',\n",
       "  '士',\n",
       "  '顿',\n",
       "  '访',\n",
       "  '问',\n",
       "  '。']]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[87,\n",
       "  109,\n",
       "  786,\n",
       "  486,\n",
       "  1194,\n",
       "  14,\n",
       "  14,\n",
       "  91,\n",
       "  14,\n",
       "  33,\n",
       "  136,\n",
       "  197,\n",
       "  89,\n",
       "  789,\n",
       "  193,\n",
       "  251,\n",
       "  5,\n",
       "  518,\n",
       "  492,\n",
       "  729,\n",
       "  109,\n",
       "  199,\n",
       "  135,\n",
       "  465,\n",
       "  813,\n",
       "  57,\n",
       "  60,\n",
       "  504,\n",
       "  4,\n",
       "  39,\n",
       "  130,\n",
       "  98,\n",
       "  22,\n",
       "  731,\n",
       "  1270,\n",
       "  304,\n",
       "  113,\n",
       "  120,\n",
       "  1234,\n",
       "  476,\n",
       "  1225,\n",
       "  259,\n",
       "  786,\n",
       "  486,\n",
       "  1194,\n",
       "  400,\n",
       "  121,\n",
       "  3]]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,   87,  109,  786,\n",
       "         486, 1194,   14,   14,   91,   14,   33,  136,  197,   89,  789,\n",
       "         193,  251,    5,  518,  492,  729,  109,  199,  135,  465,  813,\n",
       "          57,   60,  504,    4,   39,  130,   98,   22,  731, 1270,  304,\n",
       "         113,  120, 1234,  476, 1225,  259,  786,  486, 1194,  400,  121,\n",
       "           3]])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-nr': 1, 'B-ns': 2, 'B-nt': 3, 'I-nr': 4, 'I-ns': 5, 'I-nt': 6, 'O': 0}"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-187-16aa8355f9bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_val\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'list'"
     ]
    }
   ],
   "source": [
    "y_val=[1, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = model.predict(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        4, 4, 0, 1, 4, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 2, 0, 0, 0, 3, 6, 6, 6, 6, 0]], dtype=int64)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label_ids = np.argmax(test_predict,axis=2)\n",
    "test_label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_labels = dict([index, label] for (label, index) in labels_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 1)"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([48],\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         4, 4, 0, 1, 4, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 2, 0, 0, 0, 3, 6, 6, 6, 6, 0]], dtype=int64))"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths=[]\n",
    "for i in range(len(texts)):\n",
    "    lengths.append(len(texts[i]))\n",
    "lengths, test_label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load postprocess.py\n",
    "\n",
    "\n",
    "def ids_to_tags(ids, id2tag, lengths=None):\n",
    "    if lengths is None:\n",
    "        return list(map(lambda x: [id2tag.get(i) for i in x], ids))\n",
    "    else:\n",
    "        tags = []\n",
    "        for id_, length in zip(ids, lengths):\n",
    "            tags.append([id2tag.get(i) for i in id_[-length:]])\n",
    "        return tags\n",
    "\n",
    "\n",
    "def split_by_tags(sentences, tags, tag_format='BIO'):\n",
    "    \"\"\"\n",
    "\n",
    "    :param list sentences: sentences [['今', '日', '查', '房'], ['患', '者'], ....] or ['今日查房', '患者....', ...]\n",
    "    :param list tags: [['B', 'I', 'O'], ['B', 'I', 'O'], ...]\n",
    "    :param str tag_format: 'BIO' or 'BMESO'\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    if tag_format not in ['BIO', 'BMESO']:\n",
    "        raise ValueError('unsupported tag format')\n",
    "    for sentence, tag in zip(sentences, tags):\n",
    "        one_result = []\n",
    "        if isinstance(sentence, list):\n",
    "            sentence = ''.join(sentence)\n",
    "        if tag_format == 'BIO':\n",
    "            start, end = 0, 0\n",
    "            for i in range(len(tag)):\n",
    "                if tag[i][0] == 'B':\n",
    "                    start = i\n",
    "                elif tag[i][0] == 'I':\n",
    "                    if i != len(tag) - 1:\n",
    "                        if tag[i+1][0] == 'B':\n",
    "                            end = i + 1\n",
    "                            one_result.append({'word': sentence[start:end],\n",
    "                                               'start': start, 'end': end, 'tag': tag[start][2:]})\n",
    "                elif tag[i][0] == 'O':\n",
    "                    if i == 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if tag[i-1][0] == 'B' or tag[i-1][0] == 'I':\n",
    "                            end = i\n",
    "                            one_result.append({'word': sentence[start:end],\n",
    "                                               'start': start, 'end': end, 'tag': tag[start][2:]})\n",
    "                if i == len(tag)-1 and end < start:\n",
    "                    end = i + 1\n",
    "                    one_result.append({'word': sentence[start:end],\n",
    "                                       'start': start, 'end': end, 'tag': tag[start][2:]})\n",
    "        else:\n",
    "            start, end = 0, 0\n",
    "            for i, t in enumerate(tag):\n",
    "                if t[0] == 'B':\n",
    "                    start = i\n",
    "                elif t[0] == 'E':\n",
    "                    end = i + 1\n",
    "                    one_result.append({'word': sentence[start:end], 'start': start, 'end': end, 'tag': tag[start][2:]})\n",
    "                elif t[0] == 'S':\n",
    "                    start = i\n",
    "                    end = i + 1\n",
    "                    one_result.append({'word': sentence[start:end], 'start': start, 'end': end, 'tag': tag[start][2:]})\n",
    "\n",
    "        result.append(one_result)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tags = ids_to_tags(test_label_ids, index_labels, lengths=[48])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'B-nr', 2: 'B-ns', 3: 'B-nt', 4: 'I-nr', 5: 'I-ns', 6: 'I-nt', 0: 'O'}"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O',\n",
       "  'O',\n",
       "  'B-ns',\n",
       "  'I-ns',\n",
       "  'I-ns',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-nr',\n",
       "  'I-nr',\n",
       "  'I-nr',\n",
       "  'O',\n",
       "  'B-nr',\n",
       "  'I-nr',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-nr',\n",
       "  'I-nr',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-ns',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-nt',\n",
       "  'I-nt',\n",
       "  'I-nt',\n",
       "  'I-nt',\n",
       "  'I-nt',\n",
       "  'O']]"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'word': '波士顿', 'start': 2, 'end': 5, 'tag': 'ns'},\n",
       "  {'word': '陈特安', 'start': 13, 'end': 16, 'tag': 'nr'},\n",
       "  {'word': '李云', 'start': 17, 'end': 19, 'tag': 'nr'},\n",
       "  {'word': '江泽', 'start': 23, 'end': 25, 'tag': 'nr'},\n",
       "  {'word': '纽', 'start': 38, 'end': 39, 'tag': 'ns'},\n",
       "  {'word': '波士顿访问', 'start': 42, 'end': 47, 'tag': 'nt'}]]"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocess.split_by_tags(texts, test_tags, tag_format='BIO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
